{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gdown ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ef8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from ultralytics import YOLO\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import FileLink, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c14c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES_DATASET_1_ID = \"1IY6iL62T4M9BsgnPHT15MuHu83qis6e8\"\n",
    "PEOPLE_DATASET_ID = \"1AxWGhDv4rnoH-uHv3757I_D4rLa-j6U6\"\n",
    "USE_FOLDERS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6380d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_drive(file_id, output_path, is_folder=False):\n",
    "    try:\n",
    "        if is_folder:\n",
    "            url = f\"https://drive.google.com/drive/folders/{file_id}\"\n",
    "            gdown.download_folder(url, output=output_path, quiet=False, use_cookies=False)\n",
    "        else:\n",
    "            url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "            gdown.download(url, output_path, quiet=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Downloading datasets from Google Drive...\")\n",
    "\n",
    "if USE_FOLDERS:\n",
    "    download_from_drive(VEHICLES_FOLDER_1_ID, \"/kaggle/working/vehicles_dataset\", is_folder=True)\n",
    "    \n",
    "    download_from_drive(PEOPLE_FOLDER_ID, \"/kaggle/working/people_dataset\", is_folder=True)\n",
    "    \n",
    "else:\n",
    "    download_from_drive(VEHICLES_DATASET_1_ID, \"/kaggle/working/vehicles_dataset_1.zip\")\n",
    "    \n",
    "    download_from_drive(PEOPLE_DATASET_ID, \"/kaggle/working/people_dataset.zip\")\n",
    "    \n",
    "    !unzip -q /kaggle/working/vehicles_dataset_1.zip -d /kaggle/working/vehicles_dataset_1\n",
    "    !unzip -q /kaggle/working/people_dataset.zip -d /kaggle/working/people_dataset\n",
    "    \n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "print(\"\\nDatasets downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28638cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_structure(base_path, dataset_name):\n",
    "    print(f\"{dataset_name}:\")\n",
    "    \n",
    "    required_splits = [\"train\", \"valid\", \"test\"]\n",
    "    structure_valid = True\n",
    "    \n",
    "    for split in required_splits:\n",
    "        img_path = os.path.join(base_path, split, \"images\")\n",
    "        lbl_path = os.path.join(base_path, split, \"labels\")\n",
    "        \n",
    "        img_exists = os.path.exists(img_path)\n",
    "        lbl_exists = os.path.exists(lbl_path)\n",
    "        \n",
    "        img_count = len(os.listdir(img_path)) if img_exists else 0\n",
    "        lbl_count = len(os.listdir(lbl_path)) if lbl_exists else 0\n",
    "        \n",
    "        status = \"‚úÖ\" if img_exists and lbl_exists else \"‚ùå\"\n",
    "        print(f\"   {status} {split}: {img_count} images, {lbl_count} labels\")\n",
    "        \n",
    "        if not (img_exists and lbl_exists):\n",
    "            structure_valid = False\n",
    "    \n",
    "    print()\n",
    "    return structure_valid\n",
    "\n",
    "vehicles_valid_1 = check_dataset_structure(\"/kaggle/working/vehicles_dataset_1\", \"Vehicles Dataset_1\")\n",
    "vehicles_valid_2 = check_dataset_structure(\"/kaggle/input/cars-detection/Cars Detection\", \"Vehicles Dataset_2\")\n",
    "people_valid = check_dataset_structure(\"/kaggle/working/people_dataset\", \"People Dataset\")\n",
    "\n",
    "if vehicles_valid_1 and vehicles_valid_2 and people_valid:\n",
    "    print(\"All datasets have correct structure!\")\n",
    "else:\n",
    "    print(\"Warning: Some datasets may have incorrect structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb45b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_DATASET = \"/kaggle/working/combined_dataset\"\n",
    "\n",
    "VEHICLE_1_REMAP = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 0,\n",
    "    5: 0,\n",
    "    6: 0,\n",
    "    7: 0,\n",
    "    8: 0,\n",
    "    9: 0,\n",
    "    10: 0,\n",
    "    11: 0\n",
    "}\n",
    "\n",
    "VEHICLE_2_REMAP ={\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 0\n",
    "}\n",
    "\n",
    "CLASS_NAMES = ['car', 'bike']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_structure(base_path):\n",
    "    print(f\"Creating directory structure at {base_path}\")\n",
    "    \n",
    "    for sub in [\"images/train\", \"images/valid\", \"images/test\",\n",
    "                \"labels/train\", \"labels/valid\", \"labels/test\"]:\n",
    "        os.makedirs(os.path.join(base_path, sub), exist_ok=True)\n",
    "    \n",
    "    print(\"Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_dataset(src_base, dst_base, class_mapping=None, dataset_name=\"Dataset\", prefix=\"\"):\n",
    "    print(f\"\\nProcessing {dataset_name}...\")\n",
    "    \n",
    "    stats = {\"train\": 0, \"valid\": 0, \"test\": 0}\n",
    "    skipped_stats = defaultdict(lambda: defaultdict(int))\n",
    "    unmapped_classes = set()\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        src_images = os.path.join(src_base, split, \"images\")\n",
    "        dst_images = os.path.join(dst_base, \"images\", split)\n",
    "        \n",
    "        if os.path.exists(src_images):\n",
    "            image_files = [f for f in os.listdir(src_images) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG'))]\n",
    "            \n",
    "            for f in image_files:\n",
    "                src_path = os.path.join(src_images, f)\n",
    "                new_filename = f\"{prefix}{f}\"\n",
    "                dst_path = os.path.join(dst_images, new_filename)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "                stats[split] += 1\n",
    "        \n",
    "        src_labels = os.path.join(src_base, split, \"labels\")\n",
    "        dst_labels = os.path.join(dst_base, \"labels\", split)\n",
    "        \n",
    "        if os.path.exists(src_labels):\n",
    "            for f in os.listdir(src_labels):\n",
    "                if not f.endswith('.txt'):\n",
    "                    continue\n",
    "                    \n",
    "                src_file = os.path.join(src_labels, f)\n",
    "                new_filename = f\"{prefix}{f}\"\n",
    "                dst_file = os.path.join(dst_labels, new_filename)\n",
    "                \n",
    "                with open(src_file, \"r\") as fr:\n",
    "                    lines = fr.readlines()\n",
    "                \n",
    "                new_lines = []\n",
    "                \n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 5:\n",
    "                        skipped_stats[split][\"invalid_format\"] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        class_id = int(parts[0])\n",
    "                    except ValueError:\n",
    "                        skipped_stats[split][\"invalid_class_id\"] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    if class_mapping is not None:\n",
    "                        if class_id in class_mapping:\n",
    "                            parts[0] = str(class_mapping[class_id])\n",
    "                            new_lines.append(\" \".join(parts))\n",
    "                        else:\n",
    "                            unmapped_classes.add(class_id)\n",
    "                            skipped_stats[split][\"unmapped_class\"] += 1\n",
    "                    else:\n",
    "                        new_lines.append(\" \".join(parts))\n",
    "                \n",
    "                if new_lines:\n",
    "                    with open(dst_file, \"w\") as fw:\n",
    "                        fw.write(\"\\n\".join(new_lines) + \"\\n\")\n",
    "                else:\n",
    "                    skipped_stats[split][\"empty_after_filter\"] += 1\n",
    "    \n",
    "    print(f\"Train: {stats['train']} images\")\n",
    "    print(f\"Valid: {stats['valid']} images\")\n",
    "    print(f\"Test: {stats['test']} images\")\n",
    "    \n",
    "    total_skipped = sum(sum(split_stats.values()) for split_stats in skipped_stats.values())\n",
    "    if total_skipped > 0:\n",
    "        print(f\"\\nSkipped annotations:\")\n",
    "        for split, reasons in skipped_stats.items():\n",
    "            if reasons:\n",
    "                print(f\"      {split}:\")\n",
    "                for reason, count in reasons.items():\n",
    "                    print(f\"         - {reason}: {count}\")\n",
    "    \n",
    "    if unmapped_classes:\n",
    "        print(f\"\\nFound unmapped class IDs: {sorted(unmapped_classes)}\")\n",
    "            \n",
    "    return stats, skipped_stats, unmapped_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_yaml(base_path, class_names):\n",
    "    data_yaml = f\"\"\"# Dataset configuration for YOLO training\n",
    "path: {base_path}\n",
    "train: images/train\n",
    "val: images/valid\n",
    "test: images/test\n",
    "\n",
    "# Number of classes\n",
    "nc: {len(class_names)}\n",
    "\n",
    "# Class names\n",
    "names: {class_names}\n",
    "\"\"\"\n",
    "    \n",
    "    yaml_path = os.path.join(base_path, \"data.yaml\")\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        f.write(data_yaml)\n",
    "    \n",
    "    print(f\"\\ndata.yaml created at {yaml_path}\")\n",
    "    return yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd44312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_combined_dataset(base_path):\n",
    "    print(\"\\nVerifying combined dataset...\")\n",
    "    \n",
    "    total_images = 0\n",
    "    total_labels = 0\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        img_dir = os.path.join(base_path, \"images\", split)\n",
    "        lbl_dir = os.path.join(base_path, \"labels\", split)\n",
    "        \n",
    "        img_count = len([f for f in os.listdir(img_dir) \n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]) if os.path.exists(img_dir) else 0\n",
    "        lbl_count = len([f for f in os.listdir(lbl_dir) \n",
    "                        if f.endswith('.txt')]) if os.path.exists(lbl_dir) else 0\n",
    "        \n",
    "        total_images += img_count\n",
    "        total_labels += lbl_count\n",
    "        \n",
    "        print(f\"   {split.capitalize()}: {img_count} images, {lbl_count} labels\")\n",
    "        \n",
    "        if img_count != lbl_count:\n",
    "            print(f\"Warning: Mismatch in {split} set!\")\n",
    "    \n",
    "    print(f\"\\nTotal: {total_images} images, {total_labels} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cec0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_classes(dataset_path, dataset_name):\n",
    "    print(f\"\\nAnalyzing {dataset_name}...\")\n",
    "    \n",
    "    class_counts = {}\n",
    "    total_annotations = 0\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        labels_dir = os.path.join(dataset_path, split, \"labels\")\n",
    "        \n",
    "        if not os.path.exists(labels_dir):\n",
    "            continue\n",
    "            \n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            if not label_file.endswith('.txt'):\n",
    "                continue\n",
    "                \n",
    "            with open(os.path.join(labels_dir, label_file), 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        try:\n",
    "                            class_id = int(parts[0])\n",
    "                            class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "                            total_annotations += 1\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "    \n",
    "    print(f\"Total annotations: {total_annotations}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    for class_id in sorted(class_counts.keys()):\n",
    "        count = class_counts[class_id]\n",
    "        percentage = (count / total_annotations * 100) if total_annotations > 0 else 0\n",
    "        print(f\"      Class {class_id}: {count:,} annotations ({percentage:.1f}%)\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET CLASS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "vehicle_classes_1 = analyze_dataset_classes(\"/kaggle/working/vehicles_dataset_1\", \"Vehicles 1 Dataset\")\n",
    "vehicle_classes_2 = analyze_dataset_classes(\"/kaggle/input/cars-detection/Cars Detection\", \"Vehicles 2 Dataset\")\n",
    "people_classes = analyze_dataset_classes(\"/kaggle/working/people_dataset\", \"People Dataset\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if people_classes:\n",
    "    people_class_ids = set(people_classes.keys())\n",
    "    if people_class_ids != {0}:\n",
    "        print(f\"\\nPeople dataset has classes: {sorted(people_class_ids)}\")\n",
    "        print(f\"   Your person_mapping only maps class 0\")\n",
    "        print(f\"   You need to map ALL these classes to 0 (person)\")\n",
    "        print(f\"\\n   Suggested fix:\")\n",
    "        print(f\"   person_mapping = {{\")\n",
    "        for class_id in sorted(people_class_ids):\n",
    "            print(f\"       {class_id}: 0,  # person\")\n",
    "        print(f\"   }}\")\n",
    "    else:\n",
    "        print(\"People dataset looks correct (only class 0)\")\n",
    "\n",
    "if vehicle_classes_1:\n",
    "    vehicle_class_ids = set(vehicle_classes_1.keys())\n",
    "    unmapped = vehicle_class_ids - set(VEHICLE_1_REMAP.keys())\n",
    "    if unmapped:\n",
    "        print(f\"\\nVehicle dataset has unmapped classes: {sorted(unmapped)}\")\n",
    "        print(f\"   Add these to VEHICLE_REMAP\")\n",
    "\n",
    "if vehicle_classes_2:\n",
    "    vehicle_class_ids = set(vehicle_classes_2.keys())\n",
    "    unmapped = vehicle_class_ids - set(VEHICLE_2_REMAP.keys())\n",
    "    if unmapped:\n",
    "        print(f\"\\nVehicle dataset has unmapped classes: {sorted(unmapped)}\")\n",
    "        print(f\"   Add these to VEHICLE_REMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(COMBINED_DATASET):\n",
    "    shutil.rmtree(COMBINED_DATASET)\n",
    "    print(\"Removed old dataset\\n\")\n",
    "\n",
    "create_directory_structure(COMBINED_DATASET)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "veh_stats, veh_skipped, veh_unmapped = copy_dataset(\n",
    "    src_base=\"/kaggle/working/vehicles_dataset_1\",\n",
    "    dst_base=COMBINED_DATASET,\n",
    "    class_mapping=VEHICLE_1_REMAP,\n",
    "    dataset_name=\"Vehicle Dataset\",\n",
    "    prefix=\"veh1_\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "veh_stats, veh_skipped, veh_unmapped = copy_dataset(\n",
    "    src_base=\"/kaggle/input/cars-detection/Cars Detection\",\n",
    "    dst_base=COMBINED_DATASET,\n",
    "    class_mapping=VEHICLE_2_REMAP,\n",
    "    dataset_name=\"Vehicle Dataset\",\n",
    "    prefix=\"veh2_\"\n",
    ")\n",
    "\n",
    "yaml_path = create_data_yaml(COMBINED_DATASET, CLASS_NAMES)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEFORE CLEANUP\")\n",
    "print(\"=\"*70)\n",
    "verify_combined_dataset(COMBINED_DATASET)\n",
    "\n",
    "\n",
    "def clean_unmatched_images(dataset_path):\n",
    "    print(\"\\nCleaning up images without labels...\")\n",
    "    \n",
    "    total_deleted = 0\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        img_dir = os.path.join(dataset_path, \"images\", split)\n",
    "        lbl_dir = os.path.join(dataset_path, \"labels\", split)\n",
    "        \n",
    "        if not os.path.exists(img_dir):\n",
    "            continue\n",
    "        \n",
    "        deleted_count = 0\n",
    "        image_files = [f for f in os.listdir(img_dir) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img_name = os.path.splitext(img_file)[0]\n",
    "            label_file = f\"{img_name}.txt\"\n",
    "            label_path = os.path.join(lbl_dir, label_file)\n",
    "            \n",
    "            if not os.path.exists(label_path):\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                os.remove(img_path)\n",
    "                deleted_count += 1\n",
    "                total_deleted += 1\n",
    "        \n",
    "        if deleted_count > 0:\n",
    "            print(f\"   Deleted {deleted_count} images without labels in {split}\")\n",
    "    \n",
    "    print(f\"\\nTotal images deleted: {total_deleted}\")\n",
    "    return total_deleted\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "deleted = clean_unmatched_images(COMBINED_DATASET)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AFTER CLEANUP\")\n",
    "print(\"=\"*70)\n",
    "verify_combined_dataset(COMBINED_DATASET)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nCombined Dataset: {COMBINED_DATASET}\")\n",
    "print(f\"Config File: {yaml_path}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")\n",
    "print(f\"Removed: {deleted} images without labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadad007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(dataset_path, target_ratio=1.0):    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        img_dir = os.path.join(dataset_path, \"images\", split)\n",
    "        lbl_dir = os.path.join(dataset_path, \"labels\", split)\n",
    "        \n",
    "        if not os.path.exists(img_dir) or not os.path.exists(lbl_dir):\n",
    "            print(f\"\\n{split}: Directory not found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        class_0_files = []  # Images containing class 0 (people)\n",
    "        class_1_files = []  # Images containing class 1 (cars)\n",
    "        both_classes_files = []  # Images containing both classes\n",
    "        no_label_files = []  # Images with no labels or other classes\n",
    "        \n",
    "        for img_file in os.listdir(img_dir):\n",
    "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "                \n",
    "            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "            \n",
    "            if not os.path.exists(lbl_path):\n",
    "                no_label_files.append(img_file)\n",
    "                continue\n",
    "            \n",
    "            has_class_0 = False\n",
    "            has_class_1 = False\n",
    "            \n",
    "            try:\n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) > 0:\n",
    "                            class_id = int(parts[0])\n",
    "                            if class_id == 0:\n",
    "                                has_class_0 = True\n",
    "                            elif class_id == 1:\n",
    "                                has_class_1 = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {lbl_file}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if has_class_0 and has_class_1:\n",
    "                both_classes_files.append(img_file)\n",
    "            elif has_class_0:\n",
    "                class_0_files.append(img_file)\n",
    "            elif has_class_1:\n",
    "                class_1_files.append(img_file)\n",
    "            else:\n",
    "                no_label_files.append(img_file)\n",
    "        \n",
    "        print(f\"\\n{split}:\")\n",
    "        print(f\"Class 0 only (people): {len(class_0_files)} images\")\n",
    "        print(f\"Class 1 only (cars): {len(class_1_files)} images\")\n",
    "        print(f\"Both classes: {len(both_classes_files)} images\")\n",
    "        print(f\"No relevant labels: {len(no_label_files)} images\")\n",
    "        \n",
    "        total_class_0 = len(class_0_files) + len(both_classes_files)\n",
    "        total_class_1 = len(class_1_files) + len(both_classes_files)\n",
    "        \n",
    "        print(f\"Total with class 0: {total_class_0}\")\n",
    "        print(f\"Total with class 1: {total_class_1}\")\n",
    "        \n",
    "        if len(class_0_files) > len(class_1_files):\n",
    "            target_count = int(len(class_1_files) * target_ratio)\n",
    "            files_to_remove = balance_class(class_0_files, target_count, img_dir, lbl_dir)\n",
    "            print(f\"      Undersampling class 0 (people)\")\n",
    "            print(f\"      Removed: {files_to_remove} images\")\n",
    "            print(f\"      New class 0 only count: {len(class_0_files) - files_to_remove}\")\n",
    "            \n",
    "        elif len(class_1_files) > len(class_0_files):\n",
    "            target_count = int(len(class_0_files) * target_ratio)\n",
    "            files_to_remove = balance_class(class_1_files, target_count, img_dir, lbl_dir)\n",
    "            print(f\"Undersampling class 1 (cars)\")\n",
    "            print(f\"Removed: {files_to_remove} images\")\n",
    "            print(f\"New class 1 only count: {len(class_1_files) - files_to_remove}\")\n",
    "        else:\n",
    "            print(f\"Already balanced\")\n",
    "            continue\n",
    "        \n",
    "        remaining_class_0_only = len([f for f in os.listdir(img_dir) \n",
    "                                      if f in class_0_files and os.path.exists(os.path.join(img_dir, f))])\n",
    "        remaining_class_1_only = len([f for f in os.listdir(img_dir) \n",
    "                                      if f in class_1_files and os.path.exists(os.path.join(img_dir, f))])\n",
    "        \n",
    "        ratio = remaining_class_0_only / remaining_class_1_only if remaining_class_1_only > 0 else 0\n",
    "        print(f\"Final ratio (class 0 only : class 1 only) = {ratio:.2f}:1\")\n",
    "        print(f\"Images with both classes retained: {len(both_classes_files)}\")\n",
    "\n",
    "\n",
    "def balance_class(file_list, target_count, img_dir, lbl_dir):\n",
    "    if len(file_list) <= target_count:\n",
    "        return 0\n",
    "    \n",
    "    random.seed(42)\n",
    "    files_to_keep = set(random.sample(file_list, target_count))\n",
    "    files_to_remove_list = [f for f in file_list if f not in files_to_keep]\n",
    "    \n",
    "    removed_count = 0\n",
    "    for img_file in files_to_remove_list:\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        if os.path.exists(img_path):\n",
    "            os.remove(img_path)\n",
    "        \n",
    "        lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "        if os.path.exists(lbl_path):\n",
    "            os.remove(lbl_path)\n",
    "        \n",
    "        removed_count += 1\n",
    "    \n",
    "    return removed_count\n",
    "\n",
    "\n",
    "def verify_balanced_dataset(dataset_path):\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        img_dir = os.path.join(dataset_path, \"images\", split)\n",
    "        lbl_dir = os.path.join(dataset_path, \"labels\", split)\n",
    "        \n",
    "        if not os.path.exists(img_dir) or not os.path.exists(lbl_dir):\n",
    "            continue\n",
    "        \n",
    "        class_0_count = 0\n",
    "        class_1_count = 0\n",
    "        both_count = 0\n",
    "        total_images = 0\n",
    "        \n",
    "        for img_file in os.listdir(img_dir):\n",
    "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "            \n",
    "            total_images += 1\n",
    "            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "            \n",
    "            if not os.path.exists(lbl_path):\n",
    "                continue\n",
    "            \n",
    "            has_class_0 = False\n",
    "            has_class_1 = False\n",
    "            \n",
    "            with open(lbl_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) > 0:\n",
    "                        class_id = int(parts[0])\n",
    "                        if class_id == 0:\n",
    "                            has_class_0 = True\n",
    "                        elif class_id == 1:\n",
    "                            has_class_1 = True\n",
    "            \n",
    "            if has_class_0 and has_class_1:\n",
    "                both_count += 1\n",
    "            elif has_class_0:\n",
    "                class_0_count += 1\n",
    "            elif has_class_1:\n",
    "                class_1_count += 1\n",
    "        \n",
    "        print(f\"\\n{split.upper()}:\")\n",
    "        print(f\"   Total images: {total_images}\")\n",
    "        print(f\"   Class 0 only: {class_0_count}\")\n",
    "        print(f\"   Class 1 only: {class_1_count}\")\n",
    "        print(f\"   Both classes: {both_count}\")\n",
    "        print(f\"   Ratio (0:1): {class_0_count/class_1_count if class_1_count > 0 else 'N/A':.2f}\")\n",
    "\n",
    "\n",
    "COMBINED_DATASET = \"/kaggle/working/combined_dataset\"\n",
    "balance_dataset(COMBINED_DATASET, target_ratio=1.0)\n",
    "verify_balanced_dataset(COMBINED_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mixed_class_images(dataset_path, output_path, num_mixed=500, split_ratio=(0.7, 0.2, 0.1)):\n",
    "    random.seed(42)\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        img_dir = os.path.join(dataset_path, \"images\", split)\n",
    "        lbl_dir = os.path.join(dataset_path, \"labels\", split)\n",
    "        \n",
    "        out_img_dir = os.path.join(output_path, \"images\", split)\n",
    "        out_lbl_dir = os.path.join(output_path, \"labels\", split)\n",
    "        os.makedirs(out_img_dir, exist_ok=True)\n",
    "        os.makedirs(out_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        if not os.path.exists(img_dir) or not os.path.exists(lbl_dir):\n",
    "            continue\n",
    "        \n",
    "        class_images = {}\n",
    "        \n",
    "        for img_file in os.listdir(img_dir):\n",
    "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "            \n",
    "            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            lbl_path = os.path.join(lbl_dir, lbl_file)\n",
    "            \n",
    "            if not os.path.exists(lbl_path):\n",
    "                continue\n",
    "            \n",
    "            with open(lbl_path, 'r') as f:\n",
    "                classes = set()\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) > 0:\n",
    "                        classes.add(int(parts[0]))\n",
    "                \n",
    "                if len(classes) == 1:\n",
    "                    cls = list(classes)[0]\n",
    "                    if cls not in class_images:\n",
    "                        class_images[cls] = []\n",
    "                    class_images[cls].append(img_file)\n",
    "        \n",
    "        print(f\"\\n{split}:\")\n",
    "        for cls, images in sorted(class_images.items()):\n",
    "            print(f\"Class {cls} images available: {len(images)}\")\n",
    "        \n",
    "        if len(class_images) < 2:\n",
    "            print(f\"Not enough classes to create mixed samples\")\n",
    "            continue\n",
    "        \n",
    "        class_ids = sorted(class_images.keys())\n",
    "        all_combinations = []\n",
    "        \n",
    "        for combo in combinations(class_ids, 2):\n",
    "            all_combinations.append(combo)\n",
    "        \n",
    "        if len(class_ids) >= 3:\n",
    "            for combo in combinations(class_ids, 3):\n",
    "                all_combinations.append(combo)\n",
    "        \n",
    "        print(f\"Creating combinations: {all_combinations}\")\n",
    "        \n",
    "        if split == \"train\":\n",
    "            num_to_create = int(num_mixed * split_ratio[0])\n",
    "        elif split == \"valid\":\n",
    "            num_to_create = int(num_mixed * split_ratio[1])\n",
    "        else:\n",
    "            num_to_create = int(num_mixed * split_ratio[2])\n",
    "        \n",
    "        images_per_combo = max(1, num_to_create // len(all_combinations))\n",
    "        \n",
    "        created = 0\n",
    "        mix_idx = 0\n",
    "        \n",
    "        for combo in all_combinations:\n",
    "            combo_created = 0\n",
    "            \n",
    "            for i in range(images_per_combo):\n",
    "                try:\n",
    "                    selected_images = []\n",
    "                    selected_labels = []\n",
    "                    \n",
    "                    for cls in combo:\n",
    "                        if len(class_images[cls]) == 0:\n",
    "                            continue\n",
    "                        img_file = random.choice(class_images[cls])\n",
    "                        selected_images.append(os.path.join(img_dir, img_file))\n",
    "                        selected_labels.append(os.path.join(lbl_dir, os.path.splitext(img_file)[0] + '.txt'))\n",
    "                    \n",
    "                    if len(selected_images) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    if len(selected_images) == 2:\n",
    "                        mixed_img, mixed_labels = combine_two_images(\n",
    "                            selected_images[0], selected_images[1],\n",
    "                            selected_labels[0], selected_labels[1]\n",
    "                        )\n",
    "                    else:\n",
    "                        mixed_img, mixed_labels = combine_multiple_images(\n",
    "                            selected_images, selected_labels\n",
    "                        )\n",
    "                    \n",
    "                    mixed_name = f\"mixed_{split}_{mix_idx:05d}.jpg\"\n",
    "                    mixed_img.save(os.path.join(out_img_dir, mixed_name))\n",
    "                    \n",
    "                    with open(os.path.join(out_lbl_dir, f\"mixed_{split}_{mix_idx:05d}.txt\"), 'w') as f:\n",
    "                        f.write(mixed_labels)\n",
    "                    \n",
    "                    created += 1\n",
    "                    combo_created += 1\n",
    "                    mix_idx += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating mixed image: {e}\")\n",
    "            \n",
    "            print(f\"Created {combo_created} images for combination {combo}\")\n",
    "        \n",
    "        print(f\"Total created: {created} mixed-class images\")\n",
    "\n",
    "\n",
    "def combine_two_images(img0_path, img1_path, lbl0_path, lbl1_path):\n",
    "    img0 = Image.open(img0_path).convert('RGB')\n",
    "    img1 = Image.open(img1_path).convert('RGB')\n",
    "    \n",
    "    target_height = 640\n",
    "    img0 = img0.resize((int(img0.width * target_height / img0.height), target_height))\n",
    "    img1 = img1.resize((int(img1.width * target_height / img1.height), target_height))\n",
    "    \n",
    "    total_width = img0.width + img1.width\n",
    "    combined = Image.new('RGB', (total_width, target_height))\n",
    "    combined.paste(img0, (0, 0))\n",
    "    combined.paste(img1, (img0.width, 0))\n",
    "    \n",
    "    combined_labels = []\n",
    "    \n",
    "    with open(lbl0_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                cls, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                new_x_center = (x_center * img0.width) / total_width\n",
    "                new_width = (width * img0.width) / total_width\n",
    "                combined_labels.append(f\"{int(cls)} {new_x_center:.6f} {y_center:.6f} {new_width:.6f} {height:.6f}\")\n",
    "    \n",
    "    with open(lbl1_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                cls, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                new_x_center = ((x_center * img1.width) + img0.width) / total_width\n",
    "                new_width = (width * img1.width) / total_width\n",
    "                combined_labels.append(f\"{int(cls)} {new_x_center:.6f} {y_center:.6f} {new_width:.6f} {height:.6f}\")\n",
    "    \n",
    "    return combined, '\\n'.join(combined_labels)\n",
    "\n",
    "\n",
    "def combine_multiple_images(img_paths, lbl_paths):\n",
    "    images = [Image.open(p).convert('RGB') for p in img_paths]\n",
    "    \n",
    "    target_height = 640\n",
    "    \n",
    "    resized = []\n",
    "    for img in images:\n",
    "        new_width = int(img.width * target_height / img.height)\n",
    "        resized.append(img.resize((new_width, target_height)))\n",
    "    \n",
    "    if len(resized) == 3:\n",
    "        top_width = resized[0].width + resized[1].width\n",
    "        bottom_width = resized[2].width\n",
    "        total_width = max(top_width, bottom_width)\n",
    "        total_height = target_height * 2\n",
    "        \n",
    "        combined = Image.new('RGB', (total_width, total_height))\n",
    "        combined.paste(resized[0], (0, 0))\n",
    "        combined.paste(resized[1], (resized[0].width, 0))\n",
    "        combined.paste(resized[2], ((total_width - resized[2].width) // 2, target_height))\n",
    "        \n",
    "        combined_labels = []\n",
    "        \n",
    "        with open(lbl_paths[0], 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    cls, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                    new_x_center = (x_center * resized[0].width) / total_width\n",
    "                    new_y_center = (y_center * target_height) / total_height\n",
    "                    new_width = (width * resized[0].width) / total_width\n",
    "                    new_height = (height * target_height) / total_height\n",
    "                    combined_labels.append(f\"{int(cls)} {new_x_center:.6f} {new_y_center:.6f} {new_width:.6f} {new_height:.6f}\")\n",
    "        \n",
    "        with open(lbl_paths[1], 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    cls, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                    new_x_center = ((x_center * resized[1].width) + resized[0].width) / total_width\n",
    "                    new_y_center = (y_center * target_height) / total_height\n",
    "                    new_width = (width * resized[1].width) / total_width\n",
    "                    new_height = (height * target_height) / total_height\n",
    "                    combined_labels.append(f\"{int(cls)} {new_x_center:.6f} {new_y_center:.6f} {new_width:.6f} {new_height:.6f}\")\n",
    "        \n",
    "        x_offset = (total_width - resized[2].width) // 2\n",
    "        with open(lbl_paths[2], 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    cls, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                    new_x_center = ((x_center * resized[2].width) + x_offset) / total_width\n",
    "                    new_y_center = ((y_center * target_height) + target_height) / total_height\n",
    "                    new_width = (width * resized[2].width) / total_width\n",
    "                    new_height = (height * target_height) / total_height\n",
    "                    combined_labels.append(f\"{int(cls)} {new_x_center:.6f} {new_y_center:.6f} {new_width:.6f} {new_height:.6f}\")\n",
    "        \n",
    "        return combined, '\\n'.join(combined_labels)\n",
    "    \n",
    "    else:\n",
    "        return combine_two_images(img_paths[0], img_paths[1], lbl_paths[0], lbl_paths[1])\n",
    "\n",
    "\n",
    "def merge_datasets(original_path, mixed_path, output_path):\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        out_img_dir = os.path.join(output_path, \"images\", split)\n",
    "        out_lbl_dir = os.path.join(output_path, \"labels\", split)\n",
    "        os.makedirs(out_img_dir, exist_ok=True)\n",
    "        os.makedirs(out_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        orig_img_dir = os.path.join(original_path, \"images\", split)\n",
    "        orig_lbl_dir = os.path.join(original_path, \"labels\", split)\n",
    "        \n",
    "        if os.path.exists(orig_img_dir):\n",
    "            for img_file in os.listdir(orig_img_dir):\n",
    "                shutil.copy2(\n",
    "                    os.path.join(orig_img_dir, img_file),\n",
    "                    os.path.join(out_img_dir, img_file)\n",
    "                )\n",
    "        \n",
    "        if os.path.exists(orig_lbl_dir):\n",
    "            for lbl_file in os.listdir(orig_lbl_dir):\n",
    "                shutil.copy2(\n",
    "                    os.path.join(orig_lbl_dir, lbl_file),\n",
    "                    os.path.join(out_lbl_dir, lbl_file)\n",
    "                )\n",
    "        \n",
    "        # Copy mixed images\n",
    "        mixed_img_dir = os.path.join(mixed_path, \"images\", split)\n",
    "        mixed_lbl_dir = os.path.join(mixed_path, \"labels\", split)\n",
    "        \n",
    "        if os.path.exists(mixed_img_dir):\n",
    "            for img_file in os.listdir(mixed_img_dir):\n",
    "                shutil.copy2(\n",
    "                    os.path.join(mixed_img_dir, img_file),\n",
    "                    os.path.join(out_img_dir, img_file)\n",
    "                )\n",
    "        \n",
    "        if os.path.exists(mixed_lbl_dir):\n",
    "            for lbl_file in os.listdir(mixed_lbl_dir):\n",
    "                shutil.copy2(\n",
    "                    os.path.join(mixed_lbl_dir, lbl_file),\n",
    "                    os.path.join(out_lbl_dir, lbl_file)\n",
    "                )\n",
    "        \n",
    "        total_images = len(os.listdir(out_img_dir))\n",
    "        print(f\"   {split}: {total_images} total images\")\n",
    "\n",
    "\n",
    "ORIGINAL_DATASET = \"/kaggle/working/combined_dataset\"\n",
    "MIXED_OUTPUT = \"/kaggle/working/mixed_images_temp\"\n",
    "FINAL_DATASET = \"/kaggle/working/final_balanced_dataset\"\n",
    "\n",
    "create_mixed_class_images(\n",
    "    dataset_path=ORIGINAL_DATASET,\n",
    "    output_path=MIXED_OUTPUT,\n",
    "    num_mixed= 500,\n",
    "    split_ratio=(0.7, 0.2, 0.1)\n",
    ")\n",
    "\n",
    "merge_datasets(ORIGINAL_DATASET, MIXED_OUTPUT, FINAL_DATASET)\n",
    "\n",
    "print(\"\\nDataset creation complete!\")\n",
    "print(f\"New dataset location: {FINAL_DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path_fin = create_data_yaml(\"/kaggle/working/final_balanced_dataset\", CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_people_dataset(source_dataset, output_dataset):\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        src_img_dir = os.path.join(source_dataset, \"images\", split)\n",
    "        src_lbl_dir = os.path.join(source_dataset, \"labels\", split)\n",
    "        \n",
    "        dst_img_dir = os.path.join(output_dataset, \"images\", split)\n",
    "        dst_lbl_dir = os.path.join(output_dataset, \"labels\", split)\n",
    "        os.makedirs(dst_img_dir, exist_ok=True)\n",
    "        os.makedirs(dst_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        if not os.path.exists(src_img_dir):\n",
    "            continue\n",
    "        \n",
    "        copied = 0\n",
    "        for img_file in os.listdir(src_img_dir):\n",
    "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "            \n",
    "            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            src_lbl_path = os.path.join(src_lbl_dir, lbl_file)\n",
    "            \n",
    "            if not os.path.exists(src_lbl_path):\n",
    "                continue\n",
    "            \n",
    "            has_people = False\n",
    "            filtered_labels = []\n",
    "            \n",
    "            with open(src_lbl_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls = int(parts[0])\n",
    "                        if cls == 0:  # People class\n",
    "                            has_people = True\n",
    "                            filtered_labels.append(f\"0 {' '.join(parts[1:])}\\n\")\n",
    "            \n",
    "            if has_people:\n",
    "                shutil.copy2(\n",
    "                    os.path.join(src_img_dir, img_file),\n",
    "                    os.path.join(dst_img_dir, img_file)\n",
    "                )\n",
    "                \n",
    "                with open(os.path.join(dst_lbl_dir, lbl_file), 'w') as f:\n",
    "                    f.writelines(filtered_labels)\n",
    "                \n",
    "                copied += 1\n",
    "        \n",
    "        print(f\"   {split}: {copied} images with people\")\n",
    "    \n",
    "    yaml_content = f\"\"\"path: {output_dataset}\n",
    "train: images/train\n",
    "val: images/valid\n",
    "test: images/test\n",
    "\n",
    "nc: 1\n",
    "names: ['person']\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(output_dataset, \"data.yaml\"), 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    print(f\"People dataset created at: {output_dataset}\")\n",
    "\n",
    "\n",
    "def create_vehicle_dataset(source_dataset, output_dataset, vehicle_classes=[1, 2]):\n",
    "    print(\"Creating vehicle-only dataset...\")\n",
    "    \n",
    "    class_mapping = {vehicle_classes[0]: 0, vehicle_classes[1]: 1}\n",
    "    \n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        src_img_dir = os.path.join(source_dataset, \"images\", split)\n",
    "        src_lbl_dir = os.path.join(source_dataset, \"labels\", split)\n",
    "        \n",
    "        dst_img_dir = os.path.join(output_dataset, \"images\", split)\n",
    "        dst_lbl_dir = os.path.join(output_dataset, \"labels\", split)\n",
    "        os.makedirs(dst_img_dir, exist_ok=True)\n",
    "        os.makedirs(dst_lbl_dir, exist_ok=True)\n",
    "        \n",
    "        if not os.path.exists(src_img_dir):\n",
    "            continue\n",
    "        \n",
    "        copied = 0\n",
    "        for img_file in os.listdir(src_img_dir):\n",
    "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "            \n",
    "            lbl_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            src_lbl_path = os.path.join(src_lbl_dir, lbl_file)\n",
    "            \n",
    "            if not os.path.exists(src_lbl_path):\n",
    "                continue\n",
    "            \n",
    "            has_vehicle = False\n",
    "            filtered_labels = []\n",
    "            \n",
    "            with open(src_lbl_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls = int(parts[0])\n",
    "                        if cls in class_mapping:  # Vehicle class\n",
    "                            has_vehicle = True\n",
    "                            new_cls = class_mapping[cls]\n",
    "                            filtered_labels.append(f\"{new_cls} {' '.join(parts[1:])}\\n\")\n",
    "            \n",
    "            if has_vehicle:\n",
    "                shutil.copy2(\n",
    "                    os.path.join(src_img_dir, img_file),\n",
    "                    os.path.join(dst_img_dir, img_file)\n",
    "                )\n",
    "                \n",
    "                with open(os.path.join(dst_lbl_dir, lbl_file), 'w') as f:\n",
    "                    f.writelines(filtered_labels)\n",
    "                \n",
    "                copied += 1\n",
    "        \n",
    "        print(f\"   {split}: {copied} images with vehicles\")\n",
    "    \n",
    "    yaml_content = f\"\"\"path: {output_dataset}\n",
    "train: images/train\n",
    "val: images/valid\n",
    "test: images/test\n",
    "\n",
    "nc: 2\n",
    "names: ['bike', 'car']\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(output_dataset, \"data.yaml\"), 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    print(f\"Vehicle dataset created at: {output_dataset}\")\n",
    "\n",
    "\n",
    "SOURCE_DATASET = \"/kaggle/working/combined_dataset\"\n",
    "PEOPLE_DATASET = \"/kaggle/working/people_dataset\"\n",
    "VEHICLE_DATASET = \"/kaggle/working/vehicle_dataset\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîÄ SPLITTING DATASET FOR DUAL MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "create_people_dataset(SOURCE_DATASET, PEOPLE_DATASET)\n",
    "\n",
    "print()\n",
    "create_vehicle_dataset(SOURCE_DATASET, VEHICLE_DATASET, vehicle_classes=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training people detector...\")\n",
    "people_model = YOLO('yolo11s.pt')\n",
    "people_model.train(\n",
    "    data='/kaggle/working/people_dataset/data.yaml',\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='people_detector',\n",
    "    patience=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_model = YOLO('yolo11s.pt')\n",
    "vehicle_model.train(\n",
    "    data='/kaggle/working/final_balanced_dataset/data.yaml',\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='vehicle_detector',\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac586b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_to_zip = [\n",
    "    \"/kaggle/working/runs/detect/vehicle_detector\"\n",
    "]\n",
    "\n",
    "zip_filename = \"/kaggle/working/car.zip\"\n",
    "\n",
    "with ZipFile(zip_filename, 'w') as zipf:\n",
    "    for folder in folders_to_zip:\n",
    "        folder_path = Path(folder)\n",
    "        for file in folder_path.rglob(\"*\"):\n",
    "            zipf.write(file, arcname=file.relative_to(folder_path.parent))\n",
    "\n",
    "print(f\"Zip file created: {zip_filename}\")\n",
    "\n",
    "display(FileLink(zip_filename))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
